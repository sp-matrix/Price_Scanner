%sh
#!/bin/bash

# Step 1: Get the current cluster ID
CLUSTER_ID=$(python -c 'from pyspark.sql import SparkSession; spark = SparkSession.builder.getOrCreate(); print(spark.conf.get("spark.databricks.clusterUsageTags.clusterId"))')

# Verify the cluster ID
if [ -z "$CLUSTER_ID" ]; then
  echo "Error: Could not retrieve cluster ID"
  exit 1
fi

echo "Current Cluster ID: $CLUSTER_ID"

# Step 2: Define variables for the REST API call
JOB_ID=123  # Replace with the target job ID
TOKEN="dapiYourPersonalAccessToken"  # Replace with your PAT (use secrets in production)
WORKSPACE_URL="https://your-databricks-instance.cloud.databricks.com"  # Replace with your workspace URL

# Step 3: Trigger the target job with the existing cluster ID
RESPONSE=$(curl -s -X POST \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d "{\"job_id\": $JOB_ID, \"existing_cluster_id\": \"$CLUSTER_ID\"}" \
  "$WORKSPACE_URL/api/2.1/jobs/run-now")

# Check the response
if echo "$RESPONSE" | grep -q "run_id"; then
  RUN_ID=$(echo "$RESPONSE" | jq -r '.run_id')
  echo "Successfully triggered job $JOB_ID on cluster $CLUSTER_ID with run ID $RUN_ID"
else
  echo "Failed to trigger job $JOB_ID. Response: $RESPONSE"
  exit 1
fi

# Step 4: Keep the cluster alive briefly to ensure the target job starts
sleep 60  # Adjust duration as needed